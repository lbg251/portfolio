# Machine Learning Researcher

As a doctoral graduate with a background in high energy physics and extensive experience in research, teaching, and writing, I am seeking to apply my analytical, computational, and project management skills in the field of Machine Learning. My proficiency in Python and experience with generative models, combined with a passion for AI safety and excellent communication skills, position me to make meaningful contributions to cutting-edge ML projects.

#### Technical Skills: Python, Mathematica, numpy, pandas, matplotlib, PyTorch, Git, Jupyter, tableau, SQL

## Education
- Ph.D., Physics | University of Porto (_May 2017_)								       		
- M.S., Physics	| Perimeter Institute for Theoretical Physics/ University of Waterloo (_June 2012_)	 			        		
- B.A., Individualized Study | New York University (_May 2009_)


## Work Experience
**Machine Learning and AI Safety Researcher (_June 2022 - October 2023_)**
- Conducted independent research in NLP and the mechanistic interpretability of transformer models, funded by a grant from the Center for Effective Altruism's long-term future fund.
- Discovered a novel toy model to understand attention-head superposition, published on the Alignment Forum and cited by teams at Anthropic and DeepMind.

**Summer Postdoctoral Researcher, New York University (_June 2021 - September 2021_)**
- Analyzed jet tree data generated by novel probabilistic algorithms, advised by Kyle Cranmer
- Studied the effectiveness of these models for use in jet classification, with results presented at NeurIPS.

**Adjunct Professor and Academic Advisor, New York University (_September 2017 - May 2021_)**
- Developed and taught undergraduate seminars on a variety of interdisciplinary topics.
- Advised students through their academic careers, including course planning, research projects, and grant proposals.

## Projects
### An OV-Coherent Toy Model for Attention Head Superposition
[Publication](https://www.alignmentforum.org/posts/cqRGZisKbpSjgaJbc/an-ov-coherent-toy-model-of-attention-head-superposition-1#:~:text=We%20call%20this%20%E2%80%9COV%2Dcoherent,which%20it%20is%20not%20attending.)

[Code](OVCoherentToyModel.ipynb)

We present a novel toy model for attention head superposition, which is thought to be an important part of how natural language features are implemented in transformer models. Our setup enforces superposition by requiring a single destination token to attend to multiple source tokens. The information that a head needs to copy from a source token therefore depends on information elsewhere in the context, pushing the model to form interference patterns between heads attending to different tokens. 

We implemented a 1-layer, attention-only toy model with one-hot (un)embeddings using **PyTorch**, which solves this task with perfect accuracy.We find two possible mechanisms for superposition, controlled by the number of heads and the head dimension, respectively. 

When there are multiple important source tokens to attend to in the context, heads implementing interference schema will tend to learn QK circuits such that they distribute tokens amongst themselves and don’t leave crucial information unattended to. In the OV circuit, heads use these learned attention hierarchies to write to the logits of completions consistent with the tokens it attends to. A key takeaway of this work was the relationship between these two important information channels: OV circuits can extract more information from an individual token by learning to exploit the distribution of features from their QK circuits. 

In a 2-head model, one head will learn a hierarchy of source tokens that is the complete reverse of the other:

![Flipped Hierarchy Schema for Superposition between Two Heads](/clean_projects/attn_super/show_flipped_hierarchy.png)

As each head runs through its preferences, it becomes more confident about what the right completion should be, and gains an undertstanding of the broader context. Heads get more specific with their outputs on less-interesting tokens, and constructively interfere on the correct answer.

![Direct Logit Effect of Attention Head Outputs, Conditional on attending to each Source Token](/clean_projects/attn_super/head_outputs_condl_dhead_5.png)

Understanding attention superposition contributes to language model interpretability, which is an important step in making sure they are robust, reliable, and safe. 


### Exploratory Analysis of Name Mover Heads in Redwood Research's IOI Task
[Code](Superposition_toy_data.ipynb)

This project explored name-mover heads in GPT2-small. These attention heads are responsible for copying names in an input prompt of the form "John and Mary went to the pub. John bought a pint of beer for...", and are a vital part of the circuitry found by [Redwood Research to identify the Indirect-Object (IO)](https://arxiv.org/pdf/2211.00593.pdf) of these prompts. We wanted to understand why there were so many heads seemingly responsible for name-moving in this task, and whether that was a signal that they were distributing the name moving feature in superposition. 

As part of our analysis, we used **Python** and **PyTorch** to explore the Indirect-Object Identification (IOI) circuit in a broader distribution than the one in the paper. We found that the top-k logits generated by the model corresponded to names that were subtly related, suggesting that each head picks up on different aspects of names. 

We used the IOI template from above, but replaced the names according to four datasets, corresponding to “nouns”, “proper nouns”, “names that are also religious” and “names that are also places”. We analyzed the per-prompt average logit difference for each dataset, and found some interesting features of each. For example: 
1. The noun dataset favors the subject about half the time, in a pattern suggesting that these heads learn positional information on this data.
2. On religious prompts, the model shows a strong preference for the IO unless this name is "Jesus", which is seems to dislike. 
3. On prompts that contain place names, the model prefers completions that are also places. 

We visualized the logit difference across heads and layers using **TransformerLens**, and found that the name-mover heads identified in the paper have the largest activations consistently across datasets. We also found, unsurprisingly, that the pattern for the noun dataset is very different from the original pattern, while the patterns for the religious and place names are more similar. This suggests that you cannot trick the model into IOI, and that actual name moving is happening. 

![Logits difference from patching in nouns](/clean_projects/IOI_analysis/nouns_attn.png)

![Logits difference from patching in place names](/clean_projects/IOI_analysis/places_attn.png)

In terms of attention head superposition, we came up with two hypotheses for what we were seeing: 

1. Name-mover heads boost the IO and those associated with it in some way (names that are also places, for example, or biblical names), but these constructively interfere. 
2. These grouped associations ("is IO", "Is a place", "is an object") correspond to different features that are each in superposition, making any superposition contributing solely to the IO more difficult to disentangle. 

If the dataset and examples are simple enough, we only see one feature grossly represented, but this is really just the tip of the iceberg! This point underscores just how complicated natural language is, and how important it is that the language model's internal representation aligns with our own. 

### Exploratory Study of the role of LayerNorm in transformer models 

[Code](LN_SolU.ipynb)

which asks about the distribution of the LayerNorm scale and softmax denominator in SoLU models.
 “just re-normalizes a vector” is unsatisfying, since none of the non-linearities in these models are particularly complicated, and LayerNorm seems to have a large effect on training and optimization. Reproducing Conjecture’s work “Re-examining LayerNorm” made it clear that the consequences of LayerNorm are much less trivial than “just re-normalizing”, that it can be used in place of an activation function in at least some cases, and that it could have real semantic impact in transformer models. 

The presence of LayerNorm after the SoLU activation function makes this more concrete, and much more interesting. Apart from improving model performance, does this LayerNorm successfully prevent SoLU from hiding non-basis aligned features that are still present? How would this depend on input data? Should the LayerNorm after a non-linear activation like SoLU, which privileges the basis, be treated differently from LayerNorms that follow linear operations (and, in what circumstances can these be seen as another non-linear activation in the model)? 

 For inference, I used the Microsoft Research Paraphrase Dataset from HuggingFace, since it seemed reasonably sized, with 3668 examples in its training set, and not too limited in its topics that it would skew the analysis too much. 

Key Experiments/ Takeaways: 
For a single prompt token, plotting the residual stream vector gives a good idea for what each piece of the SoLU + LayerNorm is doing to each direction in feature space (see figure 1). The SoLU numerator scales and separates components, and the denominator exaggerates this separation. The LayerNorm is almost linear on its input, but moves the points around on this line (like the stretching/folding behavior in Conjecture’s post. This same shifting can be seen in other LayerNorm layers of this network.). To do: color code the points by their position in the initial vector, so I can see exactly how they move around. It would also be good to look at these plots for different inputs, since SoLU should treat diffuse and sparse activations differently.  

Since the LayerNorm effectively gets rid of the Softmax denominator, it makes sense to compare this denominator with the LayerNorm scale. The hope is that the LayerNorm will retain the monosemantic neurons isolated by the SoLU activation, even as it increases the activations of more diffusely distributed features. 

 Since the LayerNorm scale is the variance of a token’s feature vector (or the norm for “centered” models), it will be close to 0 for diffuse vectors and larger for sparse vectors. So, I think I’m convinced that looking at these values across tokens will tell us something about which are most activated by a single neuron (or at least fewer neurons). 
 
Dividing by this denominator definitely exaggerates the polarizing effect of SoLU activations.

 If I instead plot a histogram of values across all tokens for all example prompts (figure 3), the distribution looks much less noisy, and there do seem to be outlier bars that come from many examples. 

The first time I ran this model, I used prepend_BOS = True, and saw a very high bar to the right of the histograms for both the LayerNorm scale and the SoLU denominator. Thinking this might be the culprit, I set prepend_BOS=False and the bar disappeared. This probably means that some tokens are embedded in the same direction every time they appear in the model, regardless of position in the prompt! In hindsight, I think I should have expected this, but I don’t think this should be true for all tokens, since the interpretation (feature representation?) of many tokens should depend on context. It would be interesting to do a per-token analysis here to figure this out. 

![Histogram of the SoLU denominator across all tokens](/clean_projects/LayerNorm/SoLU_denominator.png)

**Python** **PyTorch** **TransformerLens** 

### A Tableau Tutorial for Analyzing iSeaTree Data 
[Tutorial](https://treemama.org/how-to-make-maps-and-tree-maps-in-tableau/)

Created a tutorial to create geographic maps, histograms, and Treemaps in **Tableau** to visualize iNaturalist tree data, as part of iSeaTree's outreach efforts to help middle and high school teachers plan lessons around biology and data analysis. 

### Computing The Exact Optimal Classifier for Ginkgo Jets
[Publication](https://ml4physicalsciences.github.io/2022/files/NeurIPS_ML4PS_2022_32.pdf)

Used **Python** and **PyTorch** 
As a community we have performed a number of comparisons of different architectures for jet tagging and other tasks. We’ve even seen that combining multiple classifiers together into a meta-tagger that had slightly better performance. But where does it end? What is the performance of the optimal tagger? Formally, the optimal classifier is defined by a likelihood ratio (Neyman-Pearson lemma), but the likelihood for the observed jet is typically intractable as it involves marginalizing over the enormous number of showering histories and the likelihood for a particular shower is also inconvenient to work with. We consider new datasets with signal and background generated with the Ginkgo model and we use the cluster trellis to exactly compute the marginal likelihood under each hypothesis in order to calculate the exact optimal likelihood ratio. We can use these results to compare the performance of ML-based taggers to this optimal classifier.
![Jet Discrimination (left) and Optimal Classification via the Neyman-Pearson Lemma (Right)](/clean_projects/Jets/optimal_classifier.png)


## Publications
1. Greenspan, L., Wynroe, K. {\it An OV-coherent toy model of attention head superposition.} Alignment Forum. 2023.
2. Cranmer, K., Drnevich, M., Greenspan, L., Macaluso, S., Pappadopulo, D. {\it Computing the Bayes-optimal classifier and exact maximum likelihood estimator with a semi-realistic generative model for jet physics.} Machine Learning and the Physical Sciences workshop, NeurIPS 2022
3. Greenspan, L. {\it Holography, Application, and String Theory's Changing Nature.} Studies of History and Philosophy of Science 2022, 94 pp. 72-86 [arXiv:2205.05159]
4. Greenspan, L. {\it The Bootstrap: Building Nature from the Bottom Up.} Inside the Perimeter. 2017 [http://insidetheperimeter.ca/bootstrap-building-nature/]
5. Costa, MS., Greenspan, L., Penedones, J., Santos, JE.  {\it Polarised Black Holes in ABJM.} J. High Energ. Phys. 2017: 24. [hep-th/1702.04353]
6. Costa, MS., Greenspan, L., Oliveira, M., Penedones, J., Santos, JE. {\it Polarised Black Holes in AdS.} Class. Quant. Grav 33 2016, 11 pp. 115011 [hep-th/1511.08505]
7. Costa, MS., Greenspan, L., Penedones, J., Santos, JE.  {\it Thermodynamics of the BMN Matrix Model at Strong Coupling. }J. High Energ. Phys. 2015, 03 pp. 69. [hep-th/1411.5541]
