# Machine Learning Researcher

With a doctorate in high energy physics and extensive experience in research, teaching, and writing, I am seeking to apply my analytical, computational, and project management skills in the field of Machine Learning. My proficiency in Python and experience with generative models, combined with a passion for AI safety and excellent communication skills, position me to make meaningful contributions to cutting-edge ML projects.

#### Technical Skills: Python, Mathematica, numpy, pandas, matplotlib, PyTorch, Git, Jupyter, tableau, SQL

## Education
- Ph.D., Physics | University of Porto (_May 2017_)								       		
- M.S., Physics	| Perimeter Institute for Theoretical Physics/ University of Waterloo (_June 2012_)	 			        		
- B.A., Individualized Study | New York University (_May 2009_)


## Work Experience
**Machine Learning and AI Safety Researcher (_June 2022 - October 2023_)**
- Conducted independent research in NLP and the mechanistic interpretability of transformer models, funded by a grant from the Center for Effective Altruism's long-term future fund.
- Discovered a novel toy model to understand attention-head superposition, published on the Alignment Forum and cited by teams at Anthropic and DeepMind.

**Summer Postdoctoral Researcher, New York University (_June 2021 - September 2021_)**
- Analyzed jet tree data generated by novel probabilistic algorithms, advised by Kyle Cranmer
- Studied the effectiveness of these models for use in jet classification, with results published in NeurIPS workshop proceedings and presented at ML4Jets.

**Adjunct Professor and Academic Advisor, New York University (_September 2017 - May 2021_)**
- Developed and taught writing and research seminars on science-based interdisciplinary topics.
- Advised 12 diverse students per semester, managed their research projects, independent studies, and internships

## Projects
### An OV-Coherent Toy Model for Attention Head Superposition
[Publication](https://www.alignmentforum.org/posts/cqRGZisKbpSjgaJbc/an-ov-coherent-toy-model-of-attention-head-superposition-1#:~:text=We%20call%20this%20%E2%80%9COV%2Dcoherent,which%20it%20is%20not%20attending.)

[Code](https://github.com/lbg251/portfolio/blob/main/clean_projects/attn_super/OVCoherentToyModel.ipynb)

Transformer models are difficult to interpret in part due to their polysemanticity; the same neuron can fire for many qualititavely different language features. One explaination of this is that neurons work together in a linear combination to acheive a given feature, a phenomenon called superposition.

This intuition holds for attention heads as well as neurons. We present a novel toy model for attention head superposition along with two possible superposition mechanisms, controlled by the number of heads and the head dimension, respectively. 

It's often useful to separate the information flowing through a transformer into two channels. One (the QK circuit) involves the Queries and Keys of the attention pattern, and dictates how the destination token attends to each source token that -- for self-attention -- appears earlier in the sequence. The other (the OV circuit) links the attention pattern with the output, and dictates how attended-to tokens affect the output logits used to predict the next token. 

Our model encourages superposition by restricting a single destination token to attend to multiple source tokens. Under this condition, heads will learn QK circuits such that they distribute tokens amongst themselves and don’t leave crucial information unattended to. In the OV circuit, a head uses its learned attention hierarchy to write to the logits of completions consistent with the tokens it attends to. The information that a head needs to copy from a source token therefore depends on information of others in the context, pushing the model to form interference patterns between heads attending to different tokens. 

We implement a 1-layer, attention-only toy model with one-hot (un)embeddings using **PyTorch**, which solves this task with perfect accuracy. Our Inputs are sequences of integers in the range [0,11], and are either noise (context and completions drawn randomly) or signal (two of the context's integers are drawn in the range [1,4], the final token is 0, and the completions are generated by an arbitrary map).

In a 2-head model, one head will learn a hierarchy of source tokens that is the complete reverse of the other. One head attends to "2" if it appears in the context, else "3", then "4", and finally "1". The second head attends to the tokens in reverse order: "1", "4", "3", and finally "2".

![Flipped Hierarchy Schema for Superposition between Two Heads](/clean_projects/attn_super/show_flipped_hierarchy.png)

As each head runs through its preferences, it becomes more confident about what the right completion should be, and gains an undertstanding of the broader context. Heads get more specific with their outputs on less-interesting tokens, and constructively interfere on the correct answer. Comparing with the above hierarchies, the following pattern shows that the first head writes positively to all possible outputs containing its top preference ("2"), and only one output containing its least favorite ("1"). For the second head, we see that this behavior is correspondingly flipped. 

![Direct Logit Effect of Attention Head Outputs, Conditional on attending to each Source Token](/clean_projects/attn_super/head_outputs_condl_dhead_5.png)

<!-- Another key takeaway of this work was the relationship between these two important information channels: OV circuits can extract more information from an individual token by learning to exploit the distribution of features from their QK circuits.  -->
Understanding attention superposition contributes to language model interpretability, which is an important step in making sure they are robust, reliable, and safe. 


### Exploratory Analysis of Name Mover Heads in Redwood Research's IOI Task
[Code](https://github.com/lbg251/portfolio/blob/main/clean_projects/IOI_analysis/Superposition_toy_data.ipynb)

This project explored name-mover heads in GPT2-small. These attention heads are responsible for copying names in an input prompt of the form "John and Mary went to the pub. John bought a pint of beer for...", and are a vital part of the circuitry found by [Redwood Research to identify the Indirect-Object (IO)](https://arxiv.org/pdf/2211.00593.pdf) of these prompts. We wanted to understand why there were so many heads seemingly responsible for name-moving in this task, and whether that was a signal that they were distributing the name moving feature in superposition. 

As part of our analysis, we used **Python** and **PyTorch** to explore the Indirect-Object Identification (IOI) circuit in a broader distribution than the one in the paper. We found that the top-k logits generated by the model corresponded to names that were subtly related, suggesting that each head picks up on different aspects of names. 

We used the IOI template from above, but replaced the names according to four datasets, corresponding to “nouns”, “proper nouns”, “names that are also religious” and “names that are also places”. We analyzed the per-prompt average logit difference for each dataset, and found some interesting features of each. For example: 
1. The noun dataset favors the subject about half the time, in a pattern suggesting that these heads learn positional information on this data.
2. On religious prompts, the model shows a strong preference for the IO unless this name is "Jesus", which is seems to dislike. 
3. On prompts that contain place names, the model prefers completions that are also places. 

![Logits difference from patching in nouns](/clean_projects/IOI_analysis/nouns_attn.png)

We visualized the logit difference across heads and layers using **TransformerLens**, and found that the name-mover heads identified in the paper have the largest activations consistently across datasets. We also found, unsurprisingly, that the pattern for the noun dataset (above) is very different from the original pattern, while the patterns for the religious and place names (below) are more similar. This suggests that you cannot trick the model into IOI, and that actual name moving is happening. 

 ![Logits difference from patching in place names](/clean_projects/IOI_analysis/places_attn.png)

In terms of attention head superposition, we came up with two hypotheses for what we were seeing: 

1. Name-mover heads boost the IO and those associated with it in some way (names that are also places, for example, or biblical names), but these constructively interfere. 
2. These grouped associations ("is IO", "Is a place", "is an object") correspond to different features that are each in superposition, making any superposition contributing solely to the IO more difficult to disentangle. 

If the dataset and examples are simple enough, we only see one feature grossly represented, but this is really just the tip of the iceberg! This point underscores just how complicated natural language is, and how important it is that the language model's internal representation aligns with our own. 

<!-- ### Exploratory Study of the role of LayerNorm in transformer models 

[Code](LN_SolU.ipynb)

In this exploratory project, I used **python**, **PyTorch**, and **TransformerLens** to study the distribution of the LayerNorm scale an softmax denominator in transformer models with a SoLU activation function. In addition to have a large effect on training and optimization, LayerNorm is a non-linear function that could impact the representation of natural language features. [SoLU models](https://transformer-circuits.pub/2022/solu/index.html) claim to make neurons more monosemantic (firing on fewer dissimilar features), and therefore more interpretable. I wanted to understand how LayerNorm impacted this interpretability. 

For a residual stream vector, the SoLU function scales and separates components in the neuron (feature) dimension. The subsequent LayerNorm exaggerates this separation by moving the vector components around in an almost linear way. Since the LayerNorm scale is the variance of a token’s feature vector, it will be close to 0 for diffuse vectors and larger for sparse vectors. The hope is that the LayerNorm will retain the monosemantic neurons isolated by the SoLU activation, even as it increases the activations of more diffusely distributed features. 

I analyzed a 1-layer pre-trained model with a hidden dimension of 512. For inference, I used the first 800 examples of the Pile, which pulls from a representative sampling of language model datasets. Since the LayerNorm effectively gets rid of the Softmax denominator, I wanted to compare these values across input tokens, to see how the neuron activations change. 

Histograms of 

 If I instead plot a histogram of values across all tokens for all example prompts (figure 3), the distribution looks much less noisy, and there do seem to be outlier bars that come from many examples. 

The first time I ran this model, I used prepend_BOS = True, and saw a very high bar to the right of the histograms for both the LayerNorm scale and the SoLU denominator. Thinking this might be the culprit, I set prepend_BOS=False and the bar disappeared. This probably means that some tokens are embedded in the same direction every time they appear in the model, regardless of position in the prompt! In hindsight, I think I should have expected this, but I don’t think this should be true for all tokens, since the interpretation (feature representation?) of many tokens should depend on context. It would be interesting to do a per-token analysis here to figure this out. 
![Histogram of the LayerNorm Scale Factor across all tokens](/clean_projects/LayerNorm/layernorm_scale.png)

![Histogram of the SoLU denominator across all tokens](/clean_projects/LayerNorm/SoLU_denominator.png)

**Python** **PyTorch** **TransformerLens**  -->

### A Tableau Tutorial for Analyzing iSeaTree Data 
[Tutorial](https://treemama.org/how-to-make-maps-and-tree-maps-in-tableau/)

Created a tutorial to create geographic maps, histograms, and Treemaps in **Tableau** to visualize iNaturalist tree data, as part of iSeaTree's outreach efforts to help middle and high school teachers plan lessons around biology and data analysis. 

### Computing The Exact Optimal Classifier for Ginkgo Jets
[Publication](https://ml4physicalsciences.github.io/2022/files/NeurIPS_ML4PS_2022_32.pdf)

Jet classification is an important machine learning task in experimental particle physics, but with costly and time-intensive explorations of better architectures and hyperparameters, finding an optimal classifier would put a useful upper-bound on performance. Formally, the Neyman-Pearson lemma defines this as a likelihood ratio between classes. However, the likelihood for an observed jet is typically intractable as it involves marginalizing over an enormous number of showering histories, and the likelihood for a particular shower is inconvenient to work with. We consider new datasets with signal and background generated with the Ginkgo model -- a simplified generative model with a tractable joint likelihood. We then use the cluster trellis to exactly compute the marginal likelihood under each hypothesis H_0 and H_1  corresponding to signal and background, respectivrely. With these tools, we can calculate the exact optimal likelihood ratio and use these results to compare the performance of ML-based taggers to this optimal classifier.

![Jet Discrimination (left) and ROC curves including the Optimal Classification via the Neyman-Pearson Lemma (Right)](/clean_projects/Jets/optimal_classifier.png)

This project is part of a larger conversation about using probabilistic methods to better inform and track inductive biases in machine learning models. In the future, I would like to integrate similar methods with NLP, a research direction that would drive progress in language model interpretability and their areas of application.

## Publications
1. [Greenspan, L., Wynroe, K. An OV-coherent toy model of attention head superposition. Alignment Forum. 2023.](https://www.alignmentforum.org/posts/cqRGZisKbpSjgaJbc/an-ov-coherent-toy-model-of-attention-head-superposition-1#:~:text=We%20call%20this%20%E2%80%9COV%2Dcoherent,which%20it%20is%20not%20attending.)
2. [Cranmer, K., Drnevich, M., Greenspan, L., Macaluso, S., Pappadopulo, D. Computing the Bayes-optimal classifier and exact maximum likelihood estimator with a semi-realistic generative model for jet physics. Machine Learning and the Physical Sciences workshop, NeurIPS 2022](https://ml4physicalsciences.github.io/2022/files/NeurIPS_ML4PS_2022_32.pdf)
3. [Greenspan, L. Holography, Application, and String Theory's Changing Nature. Studies of History and Philosophy of Science 2022, 94 pp. 72-86](https://arxiv.org/pdf/2205.05159.pdf)
4. [Greenspan, L. The Bootstrap: Building Nature from the Bottom Up. Inside the Perimeter. 2017](http://insidetheperimeter.ca/bootstrap-building-nature)
5. [Costa, MS., Greenspan, L., Penedones, J., Santos, JE. Polarised Black Holes in ABJM. J. High Energ. Phys. 2017: 24.](https://link.springer.com/content/pdf/10.1007/JHEP06(2017)024.pdf)
6. [Costa, MS., Greenspan, L., Oliveira, M., Penedones, J., Santos, JE. Polarised Black Holes in AdS.Class. Quant. Grav 33 2016, 11 pp. 115011](https://arxiv.org/pdf/1511.08505.pdf)
7. [Costa, MS., Greenspan, L., Penedones, J., Santos, JE. Thermodynamics of the BMN Matrix Model at Strong Coupling. J. High Energ. Phys. 2015, 03 pp. 69.](https://link.springer.com/content/pdf/10.1007/JHEP03%282015%29069.pdf)
